# -*- coding: utf-8 -*-
"""Using Chaikin Volatility and other Indictors

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11Cz6t8XscNxT7kiGzcmACLUrdYBJYpRu
"""

!pip install ta

!pip install pandas_ta

!rm -rf untrade-sdk  # This removes the directory
!git clone https://github.com/ztuntrade/untrade-sdk.git && cd untrade-sdk
!pip3 install ./untrade-sdk

import numpy as np
import pandas as pd
import os
import uuid
from untrade.client import Client


def process_data(data):
    """
    Process the input data to add necessary indicators and transformations
    required for signal generation.

    Parameters:
    data (pandas.DataFrame): The input data for ETH with freq '1h'.

    Returns:
    pandas.DataFrame: A dataframe with all required indicators added.
    """

    print("Starting Processing the given data--------")

    required_columns = ['high', 'low', 'close', 'volume']
    for col in required_columns:
        if col not in data.columns:
            raise ValueError(f"Missing required column: {col}")

    rsi_period = 14
    atr_period = 14
    adx_period = 14
    cv_period = 10

    # Calculate RSI
    delta = data['close'].diff()
    gain = delta.where(delta > 0, 0)
    loss = -delta.where(delta < 0, 0)

    avg_gain = gain.rolling(window=rsi_period, min_periods=1).mean()
    avg_loss = loss.rolling(window=rsi_period, min_periods=1).mean()

    rs = avg_gain / avg_loss
    data['RSI'] = 100 - (100 / (1 + rs))

    data['high_low'] = data['high'] - data['low']
    data['high_close'] = np.abs(data['high'] - data['close'].shift(1))
    data['low_close'] = np.abs(data['low'] - data['close'].shift(1))
    data['TR'] = data[['high_low', 'high_close', 'low_close']].max(axis=1)
    data['ATR'] = data['TR'].rolling(window=atr_period).mean()
    # Calculate Directional Movements (DM)
    data['+DM'] = np.where((data['high'] - data['high'].shift(1)) > (data['low'].shift(1) - data['low']),
                           data['high'] - data['high'].shift(1), 0)
    data['+DM'] = np.where(data['+DM'] < 0, 0, data['+DM'])
    data['-DM'] = np.where((data['low'].shift(1) - data['low']) > (data['high'] - data['high'].shift(1)),
                           data['low'].shift(1) - data['low'], 0)
    data['-DM'] = np.where(data['-DM'] < 0, 0, data['-DM'])

    def calculate_fibobars(data, period, fibo_level):
        highest_high = data['high'].rolling(window=period).max()
        lowest_low = data['low'].rolling(window=period).min()
        trend = []

        for i in range(len(data)):
            if i < period:
                trend.append(0)
            else:
                previous_trend = trend[-1]
                highest_h = highest_high[i]
                lowest_l = lowest_low[i]
                close = data.loc[i, 'close']
                open_price = data.loc[i, 'open']

                # Calculate trend1 and trend2
                trend1 = 1 if (previous_trend >= 0 and
                               ((highest_h - lowest_l) * fibo_level < (close - lowest_l))) else -1
                trend2 = -1 if (previous_trend <= 0 and
                                ((highest_h - lowest_l) * fibo_level < (highest_h - close))) else 1

                # # Assign trend based on candle type
                if open_price > close:
                    trend.append(trend1)
                else:
                    trend.append(trend2)
        return pd.Series(trend, index=data.index)

    data['fibo_bar'] = calculate_fibobars(data, 7, 0.618)
    data['volume_average'] = data['volume'].rolling(window=40).mean()

    # Calculate the smoothed TR, +DM, and -DM
    data['TR_smooth'] = data['TR'].rolling(window=adx_period).mean()
    data['+DM_smooth'] = data['+DM'].rolling(window=adx_period).mean()
    data['-DM_smooth'] = data['-DM'].rolling(window=adx_period).mean()

    # Calculate the +DI and -DI
    data['+DI'] = 100 * (data['+DM_smooth'] / data['TR_smooth'])
    data['-DI'] = 100 * (data['-DM_smooth'] / data['TR_smooth'])

    # Calculate the DX
    data['DX'] = np.where((data['+DI'] + data['-DI']) == 0, 100 * (np.abs(data['+DI'] - data['-DI'])), 0)
    data['DX'] = np.where(data['DX'] == 0, 100 * (np.abs(data['+DI'] - data['-DI']) / (data['+DI'] + data['-DI'])), data['DX'])

    # Calculate the ADX
    data['ADX'] = data['DX'].rolling(window=adx_period).mean()

    # Calculate Chaikin Volatility (CV) using EMA of high-low range
    data['EMA_HL'] = (data['high'] - data['low']).ewm(span=cv_period, adjust=False).mean()

    lookback_period = 10
    data['Chaikin_Volatility'] = ((data['EMA_HL'] - data['EMA_HL'].shift(lookback_period)) / data['EMA_HL'].shift(
        lookback_period)) * 100

    def calculate_psar(data, initial_af=0.0, max_af=0.2, step_af=0.02):

        # Initialize columns
        data['PSAR'] = data['low'][0]  # Start with the first low price
        data['Trend'] = 1  # 1 = Uptrend, -1 = Downtrend
        data['EP'] = data['high'][0]  # Extreme Price (EP)
        data['AF'] = initial_af  # Acceleration Factor (AF)

        for i in range(1, len(data)):
            prev_psar = data['PSAR'][i - 1]
            prev_af = data['AF'][i - 1]
            prev_ep = data['EP'][i - 1]
            prev_trend = data['Trend'][i - 1]

            # Uptrend calculation
            if prev_trend == 1:
                psar = prev_psar + prev_af * (prev_ep - prev_psar)

                # Ensure PSAR does not exceed prior two lows
                psar = min(psar, data['low'][i - 1], data['low'][i - 2] if i > 1 else data['low'][i - 1])

                # Check for trend reversal
                if data['low'][i] < psar:
                    data.loc[i, 'Trend'] = -1  # Switch to downtrend
                    data.loc[i, 'PSAR'] = prev_ep  # Set PSAR to previous EP
                    data.loc[i, 'EP'] = data['low'][i]  # Set EP to current low
                    data.loc[i, 'AF'] = initial_af  # Reset AF
                else:
                    # Continue uptrend
                    data.loc[i, 'Trend'] = 1
                    data.loc[i, 'PSAR'] = psar
                    # Update EP if current high is higher than previous EP
                    if data['high'][i] > prev_ep:
                        data.loc[i, 'EP'] = data['high'][i]
                        data.loc[i, 'AF'] = min(prev_af + step_af, max_af)
                    else:
                        data.loc[i, 'EP'] = prev_ep
                        data.loc[i, 'AF'] = prev_af

            # Downtrend calculation
            elif prev_trend == -1:
                psar = prev_psar + prev_af * (prev_ep - prev_psar)

                # Ensure PSAR does not exceed prior two highs
                psar = max(psar, data['high'][i - 1], data['high'][i - 2] if i > 1 else data['high'][i - 1])

                # Check for trend reversal
                if data['high'][i] > psar:
                    data.loc[i, 'Trend'] = 1  # Switch to uptrend
                    data.loc[i, 'PSAR'] = prev_ep  # Set PSAR to previous EP
                    data.loc[i, 'EP'] = data['high'][i]  # Set EP to current high
                    data.loc[i, 'AF'] = initial_af  # Reset AF
                else:
                    # Continue downtrend
                    data.loc[i, 'Trend'] = -1
                    data.loc[i, 'PSAR'] = psar
                    # Update EP if current low is lower than previous EP
                    if data['low'][i] < prev_ep:
                        data.loc[i, 'EP'] = data['low'][i]
                        data.loc[i, 'AF'] = min(prev_af + step_af, max_af)
                    else:
                        data.loc[i, 'EP'] = prev_ep
                        data.loc[i, 'AF'] = prev_af

        return data

    data = calculate_psar(data)

    def calculate_bbp(data, bbp_length=50):
        ema = data['close'].ewm(span=bbp_length, adjust=False).mean()
        data['BullPower'] = data['high'] - ema
        data['BearPower'] = data['low'] - ema
        data['BBP'] = data['BullPower'] + data['BearPower']
        return data

    data = calculate_bbp(data)

    def calculate_aroon(data, period):

        if 'high' not in data.columns or 'low' not in data.columns:
            raise ValueError("DataFrame must contain 'high' and 'low' columns")

        # Initialize the Aroon Up and Down columns
        aroon_up = []
        aroon_down = []

        for i in range(len(data)):
            if i < period - 1:
                # Not enough data points for calculation
                aroon_up.append(None)
                aroon_down.append(None)
                continue

            # Slice the data for the current period
            period_highs = data['high'][i - period + 1:i + 1]
            period_lows = data['low'][i - period + 1:i + 1]

            # Find the index of the highest high and lowest low
            days_since_high = period - 1 - period_highs.idxmax() + (i - period + 1)
            days_since_low = period - 1 - period_lows.idxmin() + (i - period + 1)

            # Calculate Aroon Up and Aroon Down
            aroon_up_value = ((period - days_since_high) / period) * 100
            aroon_down_value = ((period - days_since_low) / period) * 100

            aroon_up.append(aroon_up_value)
            aroon_down.append(aroon_down_value)

        # Add the calculated values to the DataFrame
        data[f'Aroon_Up_{period}'] = aroon_up
        data[f'Aroon_Down_{period}'] = aroon_down

        return data

    data = calculate_aroon(data, 56)
    data = calculate_aroon(data, 20)
    data = calculate_aroon(data, 17)
    data = calculate_aroon(data, 55)

    def calculate_ash(data, length):

        ahma = data['close'].ewm(span=length).mean() - data['close'].ewm(span=length * 2).mean()
        data['ASH'] = ahma
        return data

    data = calculate_ash(data, 9)

    def calculate_dpo(df, period=21):
        ma = df['close'].rolling(window=period).mean()
        barsback = period // 2 + 1

        df['DPO'] = df['close'] - ma.shift(barsback)
        return df

    data = calculate_dpo(data)

    data['high_low'] = data['high'] - data['low']
    data['high_close'] = np.abs(data['high'] - data['close'].shift(1))
    data['low_close'] = np.abs(data['low'] - data['close'].shift(1))

    def crossunder(series1, series2):

        return (series1 < series2) & (series1.shift(1) >= series2.shift(1))

    data['short_aroon_condition'] = crossunder(data['Aroon_Up_17'], data['Aroon_Down_55'])
    data['ups'] = 0
    data['dns'] = 0

    # Calculate consecutive up and down bars
    for i in range(1, len(data)):
        if data['close'].iloc[i] > data['close'].iloc[i - 1]:
            data.loc[i, 'ups'] = data['ups'].iloc[i - 1] + 1
        else:
            data.loc[i, 'ups'] = 0

        if data['close'].iloc[i] < data['close'].iloc[i - 1]:
            data.loc[i, 'dns'] = data['dns'].iloc[i - 1] + 1
        else:
            data.loc[i, 'dns'] = 0

    data['atr_condition'] = (data['close']) < (data['close'].shift(1) - (data['ATR'] * 0.75))

    print("Indicators calculation done successfully and returning the processed data!!")
    return data


def strat(data):
    """
    Generate trading signals based on processed data.

    Parameters:
    data(pandas.DataFrame): The input data containing indicators.

    Returns:
    pandas.DataFrame: The dataframe with 'signals' and 'trade_type' columns added.
    """

    print("Starting to mark signals ----")

    data["signals"] = 0
    data["trade_type"] = "hold"
    position = 0
    entry = 0
    fl = 0

    for i in range(24, len(data)):
        row = data.iloc[i]
        prev_row = data.iloc[i - 1]
        signal = 0

        # initially no position
        if position == 0:

            if row['Chaikin_Volatility'] > data.loc[:i, 'Chaikin_Volatility'].quantile(0.98) and row[
                'Chaikin_Volatility'] > prev_row['Chaikin_Volatility'] < data.loc[:i, 'Chaikin_Volatility'].quantile(
                    0.98) and row['close'] > row['PSAR']:
                signal = 1
                position = 1
                data.at[i, 'trade_type'] = 'long'
                data.at[i, 'signals'] = signal
                entry = row['close']

            elif data['dns'].iloc[i] >= 5 and data['short_aroon_condition'][i] and data['close'][i] < data['PSAR'][
                i] and data['atr_condition'][i]:

                sl = row['close'] + row['ATR'] * 2

                signal = -1
                position = -1
                data.at[i, 'trade_type'] = 'short'
                data.at[i, 'signals'] = signal
                entry = row['close']
                fl = 2

            elif row['Chaikin_Volatility'] > data.loc[:i, 'Chaikin_Volatility'].quantile(0.98) and row[
                'Chaikin_Volatility'] > prev_row['Chaikin_Volatility'] < data.loc[:i, 'Chaikin_Volatility'].quantile(
                    0.98) and row['close'] < row['PSAR'] and row['DPO'] > -10:

                signal = -1
                position = -1
                data.at[i, 'trade_type'] = 'short'
                data.at[i, 'signals'] = signal
                entry = row['close']
                fl = 1

        # initially long
        elif position == 1:
            if data['dns'].iloc[i] >= 5 and data['short_aroon_condition'][i] and data['close'][i] < data['PSAR'][i] and \
                    data['atr_condition'][i]:

                sl = row['close'] + row['ATR'] * 2

                signal = -2
                position = -1
                data.at[i, 'trade_type'] = 'long_reversal'
                data.at[i, 'signals'] = signal
                entry = row['close']
                fl = 2

            elif row['Chaikin_Volatility'] > data.loc[:i, 'Chaikin_Volatility'].quantile(0.98) and row[
                'Chaikin_Volatility'] > prev_row['Chaikin_Volatility'] < data.loc[:i, 'Chaikin_Volatility'].quantile(
                    0.98) and row['close'] < row['PSAR'] and row['DPO'] > -10:
                signal = -2
                position = -1
                data.at[i, 'trade_type'] = 'long_reversal'
                data.at[i, 'signals'] = signal
                entry = row['close']
                fl = 1

            elif (row['Chaikin_Volatility'] > data.loc[:i, 'Chaikin_Volatility'].quantile(0.98) and row[
                'Chaikin_Volatility'] > prev_row['Chaikin_Volatility'] < data.loc[:i, 'Chaikin_Volatility'].quantile(
                    0.98) and row['close'] < row['PSAR']) or (row['close'] < entry - 5 * row['ATR']):

                signal = -1
                position = 0
                data.at[i, 'trade_type'] = 'short'
                data.at[i, 'signals'] = signal

            else:
                data.at[i, 'trade_type'] = 'hold'

        # initially short
        elif position == -1:

            if row['Chaikin_Volatility'] > data.loc[:i, 'Chaikin_Volatility'].quantile(0.98) and row[
                'Chaikin_Volatility'] > prev_row['Chaikin_Volatility'] < data.loc[:i, 'Chaikin_Volatility'].quantile(
                    0.98) and row['close'] > row['PSAR']:

                signal = 2
                position = 1
                data.at[i, 'trade_type'] = 'short_reversal'
                data.at[i, 'signals'] = signal
                entry = row['close']

            elif fl == 1 and (row['Chaikin_Volatility'] > data.loc[:i, 'Chaikin_Volatility'].quantile(0.98) and row[
                'Chaikin_Volatility'] > data.loc[i - 1, 'Chaikin_Volatility'] < data.loc[:i,
                                                                                'Chaikin_Volatility'].quantile(0.98) and
                              row['close'] > row['PSAR']) or row['close'] > entry + 2 * row['ATR']:
                signal = 1
                position = 0
                data.at[i, 'trade_type'] = 'long'
                data.at[i, 'signals'] = signal
                entry = 0
                fl = 0

            elif fl == 2 and (data['ups'].iloc[i] >= 5 or data['close'][i] >= sl):

                signal = 1
                position = 0
                data.at[i, 'trade_type'] = 'long'
                data.at[i, 'signals'] = signal
                entry = 0
                fl = 0

            else:
                data.at[i, 'trade_type'] = 'hold'

    print("Signals marked successfully and returning the marked dataframe for backtester!!")
    return data


def perform_bactest(csv_file_path):
    """
    Perform a backtest for small-sized files.

    Parameters:
    csv_file_path (str): Path to the csv file containing backtest data.

    Returns:
    Generator Object: Backtest Results
    """

    print("Backtesting started -----")

    jupyter_id = "chemchamp2766"
    leverage = 1
    result_type = ''

    client = Client()
    result = client.backtest(
        file_path=csv_file_path,
        leverage=leverage,
        jupyter_id=jupyter_id,
        result_type=result_type,
        limit=True
    )

    return result


def perform_backtest_large_csv(csv_file_path):
    """
        Perform a backtest for large files using chunked uploads.

        Parameters:
        csv_file_path (str): Path to the CSV file .

        Returns:
        Generator Object: Backtest Results
        """

    print("Backtesting started -----")

    jupyter_id = "chemchamp2766"
    leverage = 1
    result_type = ''

    client = Client()
    file_id = str(uuid.uuid4())
    chunk_size = 90 * 1024 * 1024  # Setting chunk size to 90 MB
    total_size = os.path.getsize(csv_file_path)
    total_chunks = (total_size + chunk_size - 1) // chunk_size
    chunk_number = 0

    # For smaller files, perform a normal backtest
    if total_size <= chunk_size:
        total_chunks = 1
        result = client.backtest(
            file_path=csv_file_path,
            leverage=leverage,
            jupyter_id=jupyter_id,
            result_type=result_type,
            limit=True
        )

        return result

    # For larger files, split into chunks and process each separately
    with open(csv_file_path, "rb") as f:
        while True:
            chunk_data = f.read(chunk_size)
            if not chunk_data:
                break
            chunk_file_path = f"/tmp/{file_id}chunk{chunk_number}.csv"
            with open(chunk_file_path, "wb") as chunk_file:
                chunk_file.write(chunk_data)

            # Execute backtest on each chunk
            result = client.backtest(
                file_path=chunk_file_path,
                leverage=leverage,
                jupyter_id=jupyter_id,
                result_type=result_type,
                chunk_number=chunk_number,
                total_chunks=total_chunks,
            )

            for value in result:
                print(value)

            os.remove(chunk_file_path)
            chunk_number += 1
    return result


# Main Execution
def main():
    # Load data
    data = pd.read_csv("ETHUSDT_1h.csv")

    # Process the data
    processed_data = process_data(data)

    # Generate signals
    result_data = strat(processed_data)

    # Save results to csv
    csv_file_path = "results.csv"
    result_data.to_csv(csv_file_path, index=False)

    # Perform backtest
    backtest_result = perform_backtest_large_csv(csv_file_path)
    print(backtest_result)
    for value in backtest_result:
        print(value)


if __name__ == "__main__":
    main()

from untrade.client import Client
client = Client()

data = pd.read_csv("results.csv")
backest_file = pd.DataFrame(data[["datetime","open","high","low","close","volume","signals","trade_type"]] )
backest_file.to_csv("backtest_file.csv")
jupyter_id = "ritwij_verma"
response = client.backtest(file_path="backtest_file.csv", leverage=1, jupyter_id=jupyter_id,result_type='Q',chain=True)
last_value = None
for value in response:
  last_value = value
  print("Final backtest result:", last_value)

import plotly.graph_objects as go
import warnings
warnings.filterwarnings("ignore")

# Use the precomputed 'data' DataFrame from Cell 1
df = data.copy()

# Plot the closing price along with buy/sell signals and spikes
fig = go.Figure()

# Plot closing price
fig.add_trace(go.Scatter(
    x=df['datetime'],
    y=df['close'],
    mode='lines',
    line=dict(color='blue'),
    name='Closing Price'
))

# Add buy signals (green triangle-up markers)
fig.add_trace(go.Scatter(
    x=df.loc[df['signals'] == 1, 'datetime'],
    y=df.loc[df['signals'] == 1, 'close'],
    mode='markers',
    marker=dict(color='green', symbol='triangle-up', size=10),
    name='Buy Signal'
))

# Add sell signals (red triangle-down markers)
fig.add_trace(go.Scatter(
    x=df.loc[df['signals'] == -1, 'datetime'],
    y=df.loc[df['signals'] == -1, 'close'],
    mode='markers',
    marker=dict(color='red', symbol='triangle-down', size=10),
    name='Sell Signal'
))

# Add position closure markers (white crosses for 'close' trade_type)
fig.add_trace(go.Scatter(
    x=df.loc[df['position_closing'] == 1, 'datetime'],
    y=df.loc[df['position_closing'] == 1, 'close'],
    mode='markers',
    marker=dict(color='white', symbol='x', size=12),
    name='Close Position'
))

# Add vertical lines for spikes
for i, spike in enumerate(df['Spike']):
    if spike:  # Check if the spike is True
        fig.add_shape(
            dict(
                type="line",
                x0=df['datetime'].iloc[i],
                y0=df['close'].min(),  # Use the min value for the y0 anchor
                x1=df['datetime'].iloc[i],
                y1=df['close'].max(),  # Use the max value for the y1 anchor
                line=dict(color="purple", width=1, dash="dot"),
            )
        )

# Update layout for the plot
fig.update_layout(
    title='Closing Price with Signals, Positions, and Spikes',
    xaxis_title='DateTime',
    yaxis_title='Price',
    template='plotly_dark',
    xaxis_rangeslider_visible=False
)

# Show the plot
fig.show()

df['Chaikin Volatility'].quantile(0.98)

len(df[df['Chaikin Volatility']>100])

import pandas as pd
import matplotlib.pyplot as plt

def plot_signals(data):
    """
    Plot signals with price data. Use upward green triangles for long signals (+1)
    and downward red triangles for short signals (-1).

    Args:
        data (DataFrame): Data containing 'datetime', 'close', and 'signals' columns.
    """
    plt.figure(figsize=(50, 24))
    data2= data.copy()

    # Plot the closing prices
    plt.plot(data2['datetime'], data2['close'], label="Close Price", color='blue', linewidth=0.5)

    # Extract long and short signals
    long_signals = data2[data2['signals'] == 1]
    short_signals = data2[data2['signals'] == -1]

    # Plot the long signals (upward green triangles)
    plt.scatter(long_signals['datetime'], long_signals['close'], color='green', marker='^', label="Buy Signal", s=200)

    # Plot the short signals (downward red triangles)
    plt.scatter(short_signals['datetime'], short_signals['close'], color='red', marker='v', label="Sell Signal", s=200)


    # Add labels, legend, and grid
    plt.title("Trading Signals on Price Data", fontsize=16)
    plt.xlabel("Date", fontsize=14)
    plt.ylabel("Price", fontsize=14)
    plt.legend()
    plt.grid(alpha=0.3)

    # Display the plot
    plt.show()

# Sample Usage
if __name__ == "__main__":
    # Assuming 'result' is the DataFrame with processed data including 'datetime', 'close', and 'signals'
    data2 = pd.read_csv("backup.csv")
    backest_file['datetime'] = pd.to_datetime(backest_file['datetime'])  # Ensure datetime is in correct format
    plot_signals(backest_file)

"""Zara sa EDA hojaye

"""

import pandas as pd
import numpy as np
from sklearn.linear_model import LinearRegression
import seaborn as sns
def analyze_slopes_and_prices(data, window=120):
    """
    Analyze rolling slopes for a given window and compute statistics for
    positive and negative slopes, along with statistics for closing prices.

    Args:
        data (pd.DataFrame): DataFrame with a column 'close' for daily closing prices.
        window (int): Rolling window size to calculate slopes.

    Returns:
        dict: Statistics for slopes (positive and negative) and closing prices.
    """
    if 'close' not in data.columns:
        raise ValueError("DataFrame must contain a 'close' column.")

    # Calculate rolling slopes
    slopes = []
    for i in range(len(data) - window + 1):
        y = data['close'].iloc[i:i + window].values.reshape(-1, 1)
        x = np.arange(window).reshape(-1, 1)
        model = LinearRegression().fit(x, y)
        slopes.append(model.coef_[0][0])  # Extract the slope

    # Pad slopes with NaN to match the DataFrame size
    slopes = [np.nan] * (window - 1) + slopes
    data['slope'] = slopes

    # Filter positive and negative slopes
    positive_slopes = data['slope'][data['slope'] > 0].dropna()
    negative_slopes = data['slope'][data['slope'] < 0].dropna()
    sns.violinplot(positive_slopes)
    plt.show()
    sns.violinplot(negative_slopes)
    plt.show()
    # Compute statistics
    slope_stats = {
        'positive_slope': {
            'max': positive_slopes.max() if not positive_slopes.empty else None,
            'min': positive_slopes.min() if not positive_slopes.empty else None,
            'median': positive_slopes.median() if not positive_slopes.empty else None,
        },
        'negative_slope': {
            'max': negative_slopes.max() if not negative_slopes.empty else None,
            'min': negative_slopes.min() if not negative_slopes.empty else None,
            'median': negative_slopes.median() if not negative_slopes.empty else None,
        }
    }

    # Compute statistics for closing prices
    price_stats = {
        'max': data['close'].max(),
        'min': data['close'].min(),
        'median': data['close'].median()
    }

    return {
        'slope_stats': slope_stats,
        'price_stats': price_stats
    }


# Analyze rolling slopes and prices
results = analyze_slopes_and_prices(data, window=120)
print("Slope Statistics:", results['slope_stats'])
print("Price Statistics:", results['price_stats'])

positive = data[data['slope']>0]
negative = data[data['slope']<0]
positive['slope'] =abs(positive['slope'])
negative['slope'] =abs(negative['slope'])
positive['slope'].mean()
negative['slope'].mean()

import pandas as pd
import numpy as np

# Example data (your data should already have these columns)
# data = pd.read_csv('data.csv')  # Load your data
bullish_threshold = 1
bearish_threshold = -1

# Ensure 'datetime' is in datetime format
data['datetime'] = pd.to_datetime(data['datetime'])

# Initialize a list to store the labels
labels = []

# Iterate over the DataFrame with a rolling window of 5 days
for i in range(len(data) - 120):  # Ensure there's enough data to look 5 days ahead
    start_date = data['datetime'].iloc[i]
    end_date = data['datetime'].iloc[i + 120]

    # Get the closing prices for those dates
    start_price = data.loc[data['datetime'] == start_date, 'close'].values
    end_price = data.loc[data['datetime'] == end_date, 'close'].values

    if start_price.size > 0 and end_price.size > 0:
        # Calculate the slope of the line between the two closing prices
        time_diff = (end_date - start_date).days

        # Slope formula (change in price) / (change in time)
        slope = (end_price - start_price) / 120

        # Label the date based on the slope
        if slope > bullish_threshold:
            label = 0 #bullish
        elif slope < bearish_threshold:
            label = 1 #bearish
        else:
            label = 2 #consolidative
    else:
        label = "No Data"

    # Store the label for the current date
    labels.append((start_date, label))

# Create a DataFrame with the results
labels_df = pd.DataFrame(labels, columns=['datetime', 'market_condition'])

# Merge the original data with the market condition labels
data = pd.merge(data, labels_df, on='datetime', how='left')

# Debug: Check the column names and the first few rows of the merged data
print("Columns in the merged data:")
print(data.columns)

# Output the final DataFrame (check if 'market_condition' column is there)
print(data[['datetime', 'close', 'market_condition']])

data['ADX']=df['ADX']
data['+DI']=df['DI+']
data['-DI']=df['DI-']

print(data['market_condition'].value_counts())

data['Bear +DI'] = data['+DI'][data['market_condition']==1]
sns.violinplot(data['Bear +DI'])
plt.show()
data['Bear -DI'] = data['-DI'][data['market_condition']==1]
sns.violinplot(data['Bear -DI'])
plt.show()
data['Bear ADX'] = data['ADX'][data['market_condition']==1]
sns.violinplot(data['Bear ADX'])
plt.plot()

data['Bull +DI'] = data['+DI'][data['market_condition']==0]
sns.violinplot(data['Bull +DI'])
plt.show()
data['Bull -DI'] = data['-DI'][data['market_condition']==0]
sns.violinplot(data['Bull -DI'])
plt.show()
data['Bull ADX'] = data['ADX'][data['market_condition']==0]
sns.violinplot(data['Bull ADX'])
plt.plot()

data['Consold ADX'] = data['ADX'][data['market_condition']==2]
sns.violinplot(data['Consold ADX'])
data['Consold +DI'] = data['+DI'][data['market_condition']==2]
sns.violinplot(data['Consold +DI'])
data['Consold -DI'] = data['-DI'][data['market_condition']==2]
sns.violinplot(data['Consold -DI'])
plt.show()

print(data['Bull +DI'].mean())
print(data['Bear +DI'].mean())
print(data['Consold +DI'].mean())

data['Chaikin Volatility'] = df['Chaikin Volatility']

CV_before_bear = []
CV_before_bull = []
CV_before_consold=[]
for i in range(1,len(data)):
  if data.loc[i,'market_condition'] == 1 and data.loc[i-1,'market_condition']!=1:
    CV_before_bear.append(data.loc[i-1,'Chaikin Volatility'])
  elif data.loc[i,'market_condition'] == 0 and data.loc[i-1,'market_condition']!=0:
    CV_before_bull.append(data.loc[i-1,'Chaikin Volatility'])
  elif data.loc[i,'market_condition'] == 2 and data.loc[i-1,'market_condition']!=2:
    CV_before_consold.append(data.loc[i-1,'Chaikin Volatility'])

CV_before_bear = np.array(CV_before_bear)
CV_before_bull = np.array(CV_before_bull)
CV_before_consold = np.array(CV_before_consold)

CV_before_bear.mean()

CV_before_bull.mean()

CV_before_consold.mean()

ADX_before_bear = []
ADX_before_bull = []
ADX_before_consold=[]
for i in range(1,len(data)):
  if data.loc[i,'market_condition'] == 1 and data.loc[i-1,'market_condition']!=1:
    ADX_before_bear.append(data.loc[i-1,'ADX'])
  elif data.loc[i,'market_condition'] == 0 and data.loc[i-1,'market_condition']!=0:
    ADX_before_bull.append(data.loc[i-1,'ADX'])
  elif data.loc[i,'market_condition'] == 2 and data.loc[i-1,'market_condition']!=2:
    ADX_before_consold.append(data.loc[i-1,'ADX'])

ADX_before_bear = np.array(ADX_before_bear)
ADX_before_bear = np.array(ADX_before_bear)
ADX_before_consold = np.array(ADX_before_consold)

ADX_before_consold.mean()

ADX_before_bear.mean()

ADX_before_consold.mean()

import pandas as pd
import numpy as np
import os
import numpy as np
import pandas as pd
import pandas_ta as ta
from untrade.client import Client

client = Client()



def crossover(series1, series2):
    """Returns True when series1 crosses above series2."""
    return (series1 > series2) & (series1.shift(1) <= series2.shift(1))

def crossunder(series1, series2):
    """Returns True when series1 crosses below series2."""
    return (series1 < series2) & (series1.shift(1) >= series2.shift(1))

# def calculate_aroon(data, upper_length, lower_length):
#     """Calculate Aroon indicators."""
#     aroon = ta.aroon(data['high'], data['low'], window=upper_length))
#     data['Aroon_Up_Long'] = aroon[f'AROONU_{upper_length}']
#     data['Aroon_Down_Long'] = aroon[f'AROOND_{lower_length}']
#     data['Aroon_Up_Short'] = aroon[f'AROONU_{lower_length}']
#     data['Aroon_Down_Short'] = aroon[f'AROOND_{upper_length}']
#     return data



def calculate_aroon(data, period):
    """
    Calculates the Aroon Up and Aroon Down indicators manually.

    Parameters:
    data (pd.DataFrame): DataFrame containing 'high' and 'low' columns.
    period (int): The lookback period for Aroon calculation (default is 14).

    Returns:
    pd.DataFrame: DataFrame with 'Aroon_Up' and 'Aroon_Down' columns added.
    """
    if 'high' not in data.columns or 'low' not in data.columns:
        raise ValueError("DataFrame must contain 'high' and 'low' columns")

    # Initialize the Aroon Up and Down columns
    aroon_up = []
    aroon_down = []

    for i in range(len(data)):
        if i < period - 1:
            # Not enough data points for calculation
            aroon_up.append(None)
            aroon_down.append(None)
            continue

        # Slice the data for the current period
        period_highs = data['high'][i - period + 1:i + 1]
        period_lows = data['low'][i - period + 1:i + 1]

        # Find the index of the highest high and lowest low
        days_since_high = period - 1 - period_highs.idxmax() + (i - period + 1)
        days_since_low = period - 1 - period_lows.idxmin() + (i - period + 1)

        # Calculate Aroon Up and Aroon Down
        aroon_up_value = ((period - days_since_high) / period) * 100
        aroon_down_value = ((period - days_since_low) / period) * 100

        aroon_up.append(aroon_up_value)
        aroon_down.append(aroon_down_value)

    # Add the calculated values to the DataFrame
    data[f'Aroon_Up_{period}'] = aroon_up
    data[f'Aroon_Down_{period}'] = aroon_down

    return data


# def calculate_psar(data, initial_af=0.02, max_af=0.2, step_af=0.02):
#     """Calculates Parabolic SAR."""
#     data['PSAR'] = ta.psar(data['high'], data['low'], data['close'], af=step_af, max_af=max_af)
#     return data


def calculate_psar(data, initial_af=0.02, max_af=0.2, step_af=0.02):
    """
    Calculates the Parabolic SAR (PSAR) for a given DataFrame.

    Parameters:
    data (pd.DataFrame): DataFrame with 'High' and 'Low' columns.
    initial_af (float): Initial acceleration factor.
    max_af (float): Maximum acceleration factor.
    step_af (float): Step increment for the acceleration factor.

    Returns:
    pd.DataFrame: Original DataFrame with an added 'PSAR' column.
    """
    # Initialize columns
    data['PSAR'] = data['low'][0]  # Start with the first low price
    data['Trend'] = 1  # 1 = Uptrend, -1 = Downtrend
    data['EP'] = data['high'][0]  # Extreme Price (EP)
    data['AF'] = initial_af  # Acceleration Factor (AF)

    for i in range(1, len(data)):
        prev_psar = data['PSAR'][i - 1]
        prev_af = data['AF'][i - 1]
        prev_ep = data['EP'][i - 1]
        prev_trend = data['Trend'][i - 1]

        # Uptrend calculation
        if prev_trend == 1:
            psar = prev_psar + prev_af * (prev_ep - prev_psar)

            # Ensure PSAR does not exceed prior two lows
            psar = min(psar, data['low'][i - 1], data['low'][i - 2] if i > 1 else data['low'][i - 1])

            # Check for trend reversal
            if data['low'][i] < psar:
                data.loc[i, 'Trend'] = -1  # Switch to downtrend
                data.loc[i, 'PSAR'] = prev_ep  # Set PSAR to previous EP
                data.loc[i, 'EP'] = data['low'][i]  # Set EP to current low
                data.loc[i, 'AF'] = initial_af  # Reset AF
            else:
                # Continue uptrend
                data.loc[i, 'Trend'] = 1
                data.loc[i, 'PSAR'] = psar
                # Update EP if current high is higher than previous EP
                if data['high'][i] > prev_ep:
                    data.loc[i, 'EP'] = data['high'][i]
                    data.loc[i, 'AF'] = min(prev_af + step_af, max_af)
                else:
                    data.loc[i, 'EP'] = prev_ep
                    data.loc[i, 'AF'] = prev_af

        # Downtrend calculation
        elif prev_trend == -1:
            psar = prev_psar + prev_af * (prev_ep - prev_psar)

            # Ensure PSAR does not exceed prior two highs
            psar = max(psar, data['high'][i - 1], data['high'][i - 2] if i > 1 else data['high'][i - 1])

            # Check for trend reversal
            if data['high'][i] > psar:
                data.loc[i, 'Trend'] = 1  # Switch to uptrend
                data.loc[i, 'PSAR'] = prev_ep  # Set PSAR to previous EP
                data.loc[i, 'EP'] = data['high'][i]  # Set EP to current high
                data.loc[i, 'AF'] = initial_af  # Reset AF
            else:
                # Continue downtrend
                data.loc[i, 'Trend'] = -1
                data.loc[i, 'PSAR'] = psar
                # Update EP if current low is lower than previous EP
                if data['low'][i] < prev_ep:
                    data.loc[i, 'EP'] = data['low'][i]
                    data.loc[i, 'AF'] = min(prev_af + step_af, max_af)
                else:
                    data.loc[i, 'EP'] = prev_ep
                    data.loc[i, 'AF'] = prev_af

    return data

def calculate_ash(data, length):
    """Calculates Absolute Strength Histogram (ASH)."""
    ahma = data['close'].ewm(span=length).mean() - data['close'].ewm(span=length * 2).mean()
    data['ASH'] = ahma
    return data

# Calculate Detrended Price Oscillator (DPO)
def calculate_dpo(df, period=21, centered=False):
    ma = df['close'].rolling(window=period).mean()
    barsback = period // 2 + 1
    if centered:
        df['DPO'] = df['close'].shift(-barsback) - ma
    else:
        df['DPO'] = df['close'] - ma.shift(barsback)
    return df

def process_data(data):
    # Aroon Settings
    length_upper_long = 56
    length_lower_long = 20
    length_upper_short = 17
    length_lower_short = 55

    # ASH Settings
    ash_length = 9

    # Calculate indicators
    data = calculate_dpo(data,period=21,centered=False)
    data = calculate_aroon(data, length_upper_long)
    data = calculate_aroon(data, length_lower_long)
    data = calculate_aroon(data, length_upper_short)
    data = calculate_aroon(data, length_lower_short)
    data = calculate_psar(data)
    data = calculate_ash(data, ash_length)

    data['high_low'] = data['high'] - data['low']
    data['high_close'] = np.abs(data['high'] - data['close'].shift(1))
    data['low_close'] = np.abs(data['low'] - data['close'].shift(1))
    data['TR'] = data[['high_low', 'high_close', 'low_close']].max(axis=1)
    data['ATR'] = data['TR'].rolling(window=14).mean()
    data['short_aroon_condition'] = crossunder(data['Aroon_Up_17'], data['Aroon_Down_55'])
    return data

# def strat(data):
#     data["signals"] = 0
#     data["trade_type"] = "hold"
#     position = 0
#     entry = 0
#     long_condition = crossover(data['Aroon_Up_56'], data['Aroon_Down_20']) & (data['Aroon_Down_20']>=5)
#     long_close_condition = crossunder(data['Aroon_Up_56'], data['Aroon_Down_20'])

#     short_condition = crossunder(data['Aroon_Up_17'], data['Aroon_Down_55'])
#     # short_close_condition = crossover(data['Aroon_Up_20'], data['Aroon_Down_20'])
#     for i in range(1, len(data)):
#         # row = data.iloc[i]
#         # prev_row = data.iloc[i - 1]



#         # Long Entry
#         if position == 0 and long_condition[i] :
#             position = 1
#             # entry = row['close'][i]
#             stop_loss=data['close'][i]-2.5*data['ATR'][i]
#             data.at[i, "signals"] = 1
#             data.at[i, "trade_type"] = "long"

#         # Long Exit
#         elif position == 1 and (long_close_condition[i] or data['close'][i]<=stop_loss):
#             position = 0
#             data.at[i, "signals"] = -1
#             data.at[i, "trade_type"] = "close"

#         # # Short Entry
#         # if position == 0 and short_condition[i] and row['PSAR'] > row['close']:
#         #     position = -1
#         #     entry = row['close']
#         #     data.at[i, "signals"] = -1
#         #     data.at[i, "trade_type"] = "short"

#         # # Short Exit
#         # elif position == -1 and (short_close_condition[i] ):
#         #     position = 0
#         #     data.at[i, "signals"] = 1
#         #     data.at[i, "trade_type"] = "close"

#     return data

# if name == 'main':
#     crypto = 'ETHUSDT'
#     freq = '12h'
#     data = pd.read_csv(f'{crypto}_{freq}.csv')
#     # data=calculate_heikin_ashi(data)
#     data = process_data(data)
#     data = strat(data)
#     data.to_csv('backup.csv')

#     backtest_file = pd.DataFrame(data[["datetime", "open", "high", "low", "close", "volume", "signals", "trade_type"]])
#     backtest_file.to_csv("backtest_file.csv")

#     jupyter_id = "krishnav"
#     leverage = 1
#     response = client.backtest(file_path="backtest_file.csv", leverage=leverage, jupyter_id=jupyter_id, result_type='', chain=True)

#     last_value = None
#     for value in response:
#         last_value = value
#         print("Final backtest result:", last_value)



# Consecutive Up/Down Bars Strategy
def calculate_consecutive_strategy(data, consecutiveBarsUp=5, consecutiveBarsDown=5):
    """
    Implements the Consecutive Up/Down Strategy.

    Args:
        data (pd.DataFrame): DataFrame containing OHLCV data.
        consecutiveBarsUp (int): Number of consecutive up bars required for a long entry.
        consecutiveBarsDown (int): Number of consecutive down bars required for a short entry.

    Returns:
        pd.DataFrame: DataFrame with signals indicating long/short/exit trades.
    """
    data['ups'] = 0
    data['dns'] = 0

    # Calculate consecutive up and down bars
    for i in range(1, len(data)):
        if data['close'].iloc[i] > data['close'].iloc[i - 1]:
            data.loc[i, 'ups'] = data['ups'].iloc[i - 1] + 1
        else:
            data.loc[i, 'ups'] = 0

        if data['close'].iloc[i] < data['close'].iloc[i - 1]:
            data.loc[i, 'dns'] = data['dns'].iloc[i - 1] + 1
        else:
            data.loc[i, 'dns'] = 0

    # Strategy signals
    data['signals'] = 0
    data['atr_condition']=(data['close'])<(data['close'].shift(1)-(data['ATR']*0.75))
    current_position = 0
    for i in range(len(data)):
        if current_position == 0:
            if data['dns'].iloc[i] >= consecutiveBarsDown and data['short_aroon_condition'][i] and data['close'][i]<data['PSAR'][i] and data['atr_condition'][i] :
                data.at[i, 'signals'] = -1  # Short Entry
                current_position = -1
                stop_loss=data['close'][i]+2*data['ATR'][i]
        # elif current_position == 1:
        #     if data['dns'].iloc[i] >= consecutiveBarsDown:
        #         data.at[i, 'signals'] = -2  # Short Entry
        #         current_position = -1
        elif current_position == -1:
            if data['ups'].iloc[i] >= consecutiveBarsUp or data['close'][i]>=stop_loss:
                data.at[i, 'signals'] = 1  # Long Entry
                current_position = 0

    return data

# Main Execution
if name == "main":
    crypto = 'ETH'
    freq = '1h'
    current_path = os.getcwd()
    data_directory = f'ETHUSDT_{freq}.csv'
    data = pd.read_csv(data_directory, parse_dates=['datetime'])
    data = process_data(data)

    # Apply Consecutive Up/Down Strategy
    result = calculate_consecutive_strategy(data, consecutiveBarsUp=5, consecutiveBarsDown=5)

    # Save Results
    result.to_csv('consecutive_up_down_strategy_results.csv', index=False)
    print("Strategy applied and results saved to 'consecutive_up_down_strategy_results.csv'.")
    csv_file_path = 'consecutive_up_down_strategy_results.csv'
    result = pd.read_csv(csv_file_path)
    result = calculate_trade_type(result)
    result = result[['datetime', 'open', 'high', 'low', 'close', 'volume', 'signals', 'trade_type']]
    result.to_csv(csv_file_path, index=False)

    # Perform Backtest
    backtest_result = perform_backtest_large_csv(csv_file_path)
    for value in backtest_result:
        print(value)

